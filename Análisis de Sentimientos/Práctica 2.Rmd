---
output: pdf_document
---

\centering

![Logo UGR](imgs/logo_ugr.jpg)
\textsc{\\[1cm] \LARGE MINERÍA DE MEDIOS SOCIALES\\[1cm]}
\textsc{\Large MÁSTER EN CIENCIA DE DATOS E INGENIERÍA DE COMPUTADORES\\[1cm]}
\textsc{\Large\bfseries Práctica 1 Bloque II - Minería de Texto \\}
\noindent\rule[-1ex]{\textwidth}{3pt}
\textsc{\Large\bfseries Curso 2021-2022 \\}

\textsc{\\[1cm] \large\bfseries Autora: Lidia Sánchez Mérida \\[1cm]}
![Logo ETSIIT](imgs/etsiit_logo.png)
\textsc{\\[1cm] \large Escuela Técnica Superior de Ingenierías Informática y de Telecomunicación}\\
\textsc{\\[1cm] \large Granada, Mayo de 2022}

\pagebreak

\raggedright

# Descripción del conjunto de datos

El conjunto de datos proporcionado para este trabajo es una versión reducida del **IMDB dataset** que contiene un total de dos mil instancias y cuatro columnas. Las dos más interesantes para cubrir los objetivos de esta práctica son *Text*, que almacena los comentarios producidos por los usuarios, y *Sentiment* que reúne el sentimiento identificado para cada uno. Adicionalmente, como podemos observar en los siguientes resultados, el conjunto de datos se encuentra **balanceado** puesto que dispone del mismo número de muestras para ambas categorías.

```{r}
# Carga el conjunto de datos desde el fichero CSV
df <- read.csv("IMDb-sample.csv")
# Número de filas y columnas
dim(df)
# Columnas del dataset
colnames(df)
# Número de muestras pertenecientes a cada clase
summary(df$Sentiment)
```

# Preprocesamiento del dataset

Una de las etapas fundamentales en problemas de identificación de sentimientos consiste en utilizar técnicas de tratamiento y limpieza de documentos para considerar únicamente los términos que aporten conocimiento útil a su resolución. 

```{r message=FALSE, warning=FALSE}
# Carga la librería para trabajar con corpus de textos
library(tm)
# Carga la librería con la que preprocesar textos
library(stringr)
# Carga la librería que permite aplicar lematización
library(textstem)
# Carga la librería que permite el uso de pipelines de trabajo
library(dplyr)
```

Por lo tanto, en este trabajo se pretenden aplicar los siguientes métodos de preprocesamiento al conjunto de opiniones del dataset:

1. **Generar un corpus**: otra de las estructuras de datos más popularmente utilizadas para el tratamiento de documentos se conoce como *corpus*. Se trata de una metodología de organización de textos que facilita la aplicación de técnicas de preprocesamiento y análisis. Una de las librerías más conocidas en R para este procedimiento es la biblioteca `tm`.

2. **Normalización**: en esta segunda etapa se pretende eliminar todos aquellos caracteres y términos que no sean relevantes para la identificación de sentimientos. Así se reduce el número de recursos computacionales y temporales que se deben invertir en el uso futuro de cualquiera de las técnicas disponibles para la resolución de este problema de clasificación. Por lo tanto procedemos a suprimir **signos de puntuación, caracteres no alfabéticos y stopwords**, además de transformar los caracteres restantes a **minúsculas** evitando los posibles inconvenientes por la diferenciación entre ambos tipos de letras.

3. **Lematización**: finalmente se reemplaza cada término de todos los textos por su raíz de modo que se eliminan palabras plurales, verbos conjugados, entre otros conceptos. Con esta técnica se pretende simplificar el análisis morfológico de un texto con el que identificar el sentimiento que representa.

```{r message=FALSE, warning=FALSE}
# Convierte la columna de opiniones a un vector fuente para generar un corpus
text_corpus <- Corpus(VectorSource(df$Text))
# Elimina signos de puntuación
clean_text_corpus <- tm_map(text_corpus, removePunctuation)
# Elimina caracteres no alfanuméricos
remove_non_alnum <- content_transformer(function(x) gsub("[^[:alnum:] ]", "", x))
clean_text_corpus <- tm_map(clean_text_corpus, remove_non_alnum)
# Elimina dígitos
clean_text_corpus <- tm_map(clean_text_corpus, removeNumbers)
# Convierte todos los caracteres a minúsculas
clean_text_corpus <- tm_map(clean_text_corpus, content_transformer(tolower))
# Elimina stopwords en inglés
clean_text_corpus <- tm_map(clean_text_corpus, 
                      content_transformer(removeWords), stopwords("english"))
# Elimina espacios extra
clean_text_corpus <- tm_map(clean_text_corpus, stripWhitespace)
# Lematización
clean_text_corpus <- tm_map(clean_text_corpus, lemmatize_strings)
# Muestra las diferencias entre el primer documento original y su versión
# preprocesada
substr(text_corpus[[1]]$content, 1, 70)
substr(clean_text_corpus[[1]]$content, 1, 70)
```

# Detección de sentimientos con diccionarios

Los diccionarios son estructuras de datos que almacenan un conjunto de **palabras junto con sus sentimientos** asociados. Su uso consiste en sumar las etiquetas de cada uno de los términos de un texto para obtener el sentimiento final del documento. Se caracterizan por una mayor velocidad de aplicación, aunque se encuentran restringidos a la terminología disponible, dejando sin etiqueta a aquellas palabras no contempladas.

A continuación presentamos dos funciones propiamente implementadas orientadas a la **predicción de sentimientos basado en un conjunto de textos y al preprocesamiento requerido para el uso de diccionarios** en la resolución de este problema de clasificación. La implementación de la función `predict_from_dict` consiste en intentar identificar los sentimientos del conjunto de documentos preprocesado a partir de las listas de términos y sentimientos proporcionadas. El principal objetivo consiste en definir un procedimiento común al reconocimiento de sentimientos utilizando diversos diccionarios. Su núcleo reside en transformar las opiniones del dataset en **vectores de palabras** con los que poder realizar operaciones de **intersección** para buscar los términos en la lista de palabras del diccionario y obtener los sentimientos asociados. Finalmente se asigna el **sentimiento más votado** entre las palabras comunes a sendas entidades, ignorando la etiqueta *neutral* puesto que el rango de valores relativos a la columna *Sentiment* exclusivamente consideran los sentimientos positivos o negativos.

```{r message=FALSE, warning=FALSE}
# Función que permite generar predicciones sobre el conjunto de documentos de
# IMDB dataset utilizando la lista de términos y sentimientos de un diccionario
# Parámetros de entrada: dos listas con los términos y sus sentimientos.
# Devuelve un diccionario con las predicciones, el número de términos totales
# de los documentos y el número de palabras encontradas en el diccionario
predict_from_dict <- function(lex_words, lex_sents) {
  # Número de palabras totales en todos los documentos
  n_tot_words <- 0
  # Número de palabras coincidentes en los documentos y en el diccionario
  n_found_words <- 0
  # Vector que almacena las predicciones del sentimiento de cada documento
  preds <- c()
  # Iteramos sobre todos los documentos del corpus
  for (i in c(1:length(clean_text_corpus))) {
    # Crea un vector de palabras para cada documento
    words_array <- unlist(strsplit(clean_text_corpus[[i]]$content, " "))
    n_tot_words <- n_tot_words + length(words_array)
    # Busca los términos comunes entre cada documento y el diccionario
    found_words <- intersect(words_array, lex_words)
    n_found_words <- n_found_words + length(found_words)
    # Obtiene los sentimientos de los términos encontrados en el diccionario
    found_sentiments <- unlist(sapply(c(1:length(found_words)), 
         function(x) lex_sents[which(found_words[x] == lex_words)]))
    # Obtiene la clase mayoritaria
    max_class <- names(which.max(table(found_sentiments, exclude='neutral')))
    # Adapta el sentimiento al rango de valores del dataset
    preds <- append(preds, ifelse(max_class == 'positive', 'POS', 'NEG'))
  }
  # Devuelve una lista con las predicciones sobre los documentos, el número
  # total de palabras de todos los textos y el número de coincidencias con
  # el diccionario
  return(list("preds"=preds, 
              "n_tot_words"=n_tot_words, 
              "n_found_words"=n_found_words))
}
```

El cometido de la segunda función denominada `preprocess_dict` consiste en aplicar el mismo **proceso de lematización** que ha sufrido el volumen de comentarios anteriormente. Existen dos principales razones explicativas que apoyan esta teoría:

* Una primera explicación reside en la intersección que se realiza entre ambas entidades para buscar los vocablos comunes y recuperar sus sentimientos. Únicamente coincidirán aquellos términos **exactamente iguales**, por lo que si una palabra se encuentra en formato raíz en un documento mientras que en el diccionario está conjugada o en plural, no será analizada y por lo tanto es información perdida para resolver el problema de clasificación.

* Por otro lado, parece comprensible que el sentimiento asociado a un término sea **independiente de la forma en la que se encuentre expresado**, por lo que si consideramos solamente las raíces de cada uno de los vocablos, podremos reducir el volumen del diccionario además de recursos computacionales y temporales. 

```{r}
# Función que aplica un proceso de lematización sobre el conjunto de palabras
# proporcionado para componer un dataset que represente la información 
# recopilada por un diccionario.
# Parámetros de entrada: dos listas con los términos y sus sentimientos.
# Devuelve un dataframe con la lista de términos originales, una segunda con
# los vocablos lematizados y sus sentimientos asociados.
preprocess_dict <- function(words, sents) {
  # Genera un corpus con los términos del diccionario
  dict_corpus <- Corpus(VectorSource(words))
  # Lematiza todos los términos del diccionario
  clean_dict_corpus <- tm_map(dict_corpus, lemmatize_strings)
  # Obtiene la lista de términos lematizados
  lem_words <- sapply(c(1:length(clean_dict_corpus)), 
                               function(x) clean_dict_corpus[[x]]$content)
  # Genera un dataset para almacenar toda la información
  dict_df <- data.frame("words"=words, 
                        "sents"=sents,
                        "lem_words"=lem_words)
  # Elimina filas en base a términos duplicados 
  dict_df <- distinct(dict_df)
  # Devuelve un dataset con el diccionario preprocesado
  return(dict_df)
}
```

## MPQA Subjectivity Lexicon

El primer procedimiento que se emplea para reconocer las etiquetas asignadas a los comentarios del dataset se encuentra ubicado dentro de la categoría de diccionarios. En el fichero *subjclueslen1-HLTEMNLP05.tff*, proporcionado en los ejemplos de la asignatura, se encuentran las **listas de términos y sentimientos** que contempla este diccionario. Por lo tanto, el primer paso consiste en cargar y almacenar la información de este archivo en dos estructuras de datos respetando el orden de los elementos. Tras visualizar brévemente el contenido del diccionario, he podido comprobar la existencia de **términos compuestos** por varias palabras unidas por guiones. Probablemente se trata de expresiones cuyo objetivo consiste en mejorar la precisión con la que se reconocen los sentimientos en los documentos. Sin embargo, para explotar esta información parece necesario realizar **análisis sintácticos y morfológicos** de todos los textos con los que obtener sus expresiones en el mismo formato. Este proceso no suele ser factible debido al volumen de datos que suele ser característico de los problemas reales y a su desmesurada inversión de recursos requeridos. Por lo tanto, procedemos a **eliminar las expresiones** del diccionario **lematizando los términos** resultantes del filtrado para establecer la misma sintaxis que el conjunto de textos reduciendo hasta mil cuatrocientos vocablos. Adicionalmente, en la última tabla se representa la cantidad de ejemplos de las diferentes clases almacenadas en el diccionario y un aspecto destacable reside en el desequilibrio del conjunto de datos siendo la **clase negativa la categoría mayoritaria**. Una consecuencia directa de este fenómeno es el aumento de la probabilidad de encontrar más términos negativos que positivos, lo que puede afectar en el rendimiento de la clasificación de los textos.

```{r message=FALSE, warning=FALSE}
# Lee el fichero que contiene el diccionario MPQA Subjectivity Lexicon
# sin cabecera y estableciendo como separador el espacio en blanco
mpqa_lex <- read.delim("./dicts/subjclueslen1-HLTEMNLP05.tff", header=FALSE, sep=" ")
# Obtiene los términos almacenados en el diccionario
mpqa_lex_words <- c(substr(mpqa_lex[, 3],
                         unlist(gregexpr('=', mpqa_lex[, 3]))+1,
                         nchar(as.character(mpqa_lex[, 3]))))
# Obtiene los sentimientos relativos al conjunto de términos anterior
mpqa_lex_sentiments <- c(substr(mpqa_lex[, 6],
                         unlist(gregexpr('=', mpqa_lex[, 6]))+1,
                         nchar(as.character(mpqa_lex[, 6]))))
# Genera un dataset con la lista de términos y sentimientos
mpqa_df <- data.frame("words"=mpqa_lex_words, 
                        "sents"=mpqa_lex_sentiments)
# Elimina los términos compuestos por varias palabras unidas por guiones
mpqa_df <- mpqa_df %>% filter(str_detect(words, "-", negate=TRUE))
# Lematiza los términos del diccionario y lo convierte en un dataset
mpqa_df <- preprocess_dict(mpqa_df$words, mpqa_df$sents)
# Número de términos del diccionario original
length(mpqa_lex_words)
# Número de términos del diccionario lematizado
length(mpqa_df$lem_words)
# Proporción de términos de las distintas categorías
summary(mpqa_df$sents)
```

Una vez disponemos del diccionario preprocesado, a continuación generamos las predicciones sobre el conjunto total de documentos utilizando la función definida anteriormente, proporcionando la lista de términos lematizados y sus sentimientos asociados. En los resultados visualizados podemos apreciar, en primer lugar, una tabla con el número de ejemplos predichos para cada clase, siendo la **positiva la categoría predominante**. La siguiente cifra muestra el porcentaje de términos encontrados en el diccionario para la totalidad de los documentos, que al tratarse únicamente de un 24.55% podemos determinar que la **mayoría de los conceptos no han sido analizados** por su inexistencia en este recurso. Como consecuencia se ha obtenido un **69.55% de precisión** tras la comparación de las etiquetas reales y las predichas sobre el conjunto de documentos completo. Por un lado estos resultados demuestran el riesgo de utilizar este tipo de estructuras para resolver problemas de detección de sentimientos, puesto que es altamente **complicado que contenga todos los términos** disponibles en un idioma. Sin embargo, tras conocer que el porcentaje de conceptos identificados apenas ha superado el 24%, el hecho de haber obtenido un 69% aproximadamente de precisión tampoco parece ser un pésimo resultado.

```{r}
# Calcula las predicciones de los documentos utilizando las listas de términos
# y sentimientos del diccionario
mpqa_results <- predict_from_dict(mpqa_df$lem_words, mpqa_df$sents)
# Resumen de predicciones
table(mpqa_results$preds)
# Calcula el porcentaje de palabras encontradas en el diccionario
mpqa_results$n_found_words/mpqa_results$n_tot_words
# Calcula la precisión comparando las predicciones con las etiquetas reales
sum(mpqa_results$preds == df$Sentiment)/length(mpqa_results$preds)
```

## SentiWordNet 3.0

Este segundo diccionario se encuentra almacenado en un fichero de texto plano denominado *SentiWordNet_3.0.0_20130122.txt* y, como el caso anterior, se encuentra ubicado en los ejemplos de la asignatura. Debido a la no estructuración del conjunto de datos que contiene, su preprocesamiento ha sido considerablemente costoso tanto temporal como computacionalmente. Tras experimentar con diferentes funciones, únicamente con `readLines` se ha conseguido obtener los términos y sentimientos asociados, a partir de la puntuación positiva y negativa de cada texto que representan el grado de pertenencia a ambos sentimientos. A semejanza del diccionario anterior, SentiWordNet 3.0 también dispone de **expresiones** compuestas por varios términos combinados mediante diversos caracteres, como el guión tradicional y el guión bajo. De nuevo, repetimos el preprocesamiento anterior en el que se **elimina la composición de palabras y se lematizan los vocablos restantes** para reducir el conjunto de datos a aquellos términos individuales que puedan ser emparejados con los conceptos almacenados en el corpus de documentos. En los siguientes resultados podemos apreciar una importante disminución del conjunto de datos almacenado en el diccionario preprocesado, aunque el aspecto más destacable es su **importante desequilibrio entre las clases existentes, siendo la clase negativa la mayoritaria** con aproximadamente un 20% de diferencia con su opuesta.

```{r message=FALSE, warning=FALSE}
# Lee el fichero del diccionario SentiWordNet 3.0 línea por línea
sent3_lex <- readLines('./dicts/SentiWordNet_3.0.0_20130122.txt')
# Preprocesamiento del diccionario para obtener los términos y sentimientos
prep_sent3_lex <- sapply(c(28:length(sent3_lex)), function(x) {
  # Separa cada columna por el caracter tabulador
  splitted_line <- unlist(strsplit(sent3_lex[x], "\t"))
  # Obtiene el sentimiento de los términos de cada línea
  # Si es mayor el score positivo, el sentimiento es positivo, si no es negativo
  sent <- ifelse(as.double(splitted_line[3]) > 
                   as.double(splitted_line[4]), 'positive', 'negative')
  # Separa la lista de términos de cada línea por el caracter espacio
  term_list <- unlist(strsplit(splitted_line[5], " "))
  # Vector que almacena los términos y sentimientos del diccionario
  sent3_data <- c()
  # Cada concepto preprocesado con su término
  sapply(c(1:length(term_list)), function(x) {
    sent3_data <- append(sent3_data, list(substr(term_list[x], 1, 
                        unlist(gregexpr('#', term_list[x]))-1), sent))
  })
})
# Genera un dataset con la lista de términos y sentimientos
sent3_df <- data.frame("words"=
                         unlist(sapply(c(1:length(prep_sent3_lex)), 
                                       function(x) prep_sent3_lex[[x]][1])), 
                        "sents"=
                         unlist(sapply(c(1:length(prep_sent3_lex)),
                                       function(x) prep_sent3_lex[[x]][2])))
# Elimina los términos compuestos por varias palabras unidas por guiones
sent3_df <- sent3_df %>% 
            filter(str_detect(words, "_", negate=TRUE)) %>% 
            filter(str_detect(words, "-", negate=TRUE))
# Lematiza los términos del diccionario y lo convierte en un dataset
sent3_df <- preprocess_dict(sent3_df$words, sent3_df$sents)
# Número de términos del diccionario original
length(prep_sent3_lex)
# Número de términos del diccionario lematizado
length(sent3_df$lem_words)
# Proporción de términos de las distintas categorías
summary(sent3_df$sents)
```

Una vez disponemos de la versión del diccionario SentiWordNet 3.0 compatible con el preprocesamiento realizado sobre el corpus de textos, hacemos uso de la función definida `predict_from_dict`, proporcionando la lista de términos individuales preprocesados y sus sentimientos asociados. En el primer resultado que se puede visualizar podemos apreciar que **todos los documentos han sido clasificados como negativos**. Este fenómeno parece ser la máxima consecuencia que puede conllevar el enorme desequilibrio del dataset, tal y como se ha destacado en su respectivo análisis. A pesar de la pésima clasificación efectuada, se han encontrado **más de la mitad de los términos** de los documentos, aunque se han tachado de negativos en su totalidad a partir del conocimiento almacenado en el diccionario preprocesado. Finalmente, la explicación relativa al valor de la tasa de aciertos puede ser debido, por un lado al equilibrio de las clases que demuestra el dataset de opiniones, y al etiquetado masivo utilizando la clase mayoritaria.

```{r}
# Calcula las predicciones de los documentos utilizando las listas de términos
# y sentimientos del diccionario
sent3_results <- predict_from_dict(sent3_df$lem_words, sent3_df$sents)
# Resumen de predicciones
table(sent3_results$preds)
# Calcula el porcentaje de palabras encontradas en el diccionario
sent3_results$n_found_words/sent3_results$n_tot_words
# Calcula la precisión comparando las predicciones con las etiquetas reales
sum(sent3_results$preds == df$Sentiment)/length(sent3_results$preds)
```

## SenticNet 5.0

El último diccionario propuesto para emplear en este trabajo se denomina *SenticNet 5.0* y su contenido se localiza dentro de un fichero nombrado como *senticnet5.txt*, también proporcionado en la asignatura. Tras varios experimentos, la función `read.table` obtiene su esquema principal generando un dataset con tres columnas. Existen dos principales diferencias a destacar con respecto a los dos anteriores diccionarios:

- La primera columna dispone de una mayor variabilidad de valores puesto que, además de almacenar términos independientes también es capaz de albergar **expresiones compuestas por varias palabras**, entre las que se encuentran diferentes adverbios que ayudan al reconocimiento de sentimientos en textos.

- A raíz de la característica anterior, la tercera columna representa la **intensidad del sentimiento** asociado a cada expresión con un rango de valores entre 0 y 1, siendo este último el máximo. Esta información adicional permite analizar la intensidad del sentimiento como si se tratase de la fiabilidad asociada a la predicción. Presumiblemente a mayor valor, mejor representado se encuentra el sentimiento identificado en un texto.

```{r warning=FALSE}
# Carga el diccionario desde el fichero TXT como una tabla para separar 
# las columnas
sent5_lex <- read.table("./dicts/senticnet5.txt", header=TRUE)
head(sent5_lex)
```

Si bien las dos cualidades destacadas anteriormente podrían proporcionar resultados más precisos al analizar la composición de un conjunto de documentos, requieren un **análisis sintáctico y morfológico notablemente complejo** y personalizado para adaptar la terminología al diccionario, como ocurría en casos anteriores.  Por la alta inversión de recursos temporales que implica este procedimiento, de nuevo eliminamos las expresiones del diccionario para considerar únicamente aquellos términos que son útiles con respecto al corpus preprocesado. Finalmente aplicamos el mismo proceso de **lematización** como se efectuó en los diccionarios previos con el fin de establecer la misma sintaxis que los documentos y continuar reduciendo el número de palabras del diccionario. De este modo el conjunto de términos disponibles se ha reducido en más de un 60%, tal y como se puede apreciar en los siguientes resultados. Una de las diferencias más notables con respecto a los dos diccionarios previos es su **equilibrio entre el número de conceptos positivos y negativos** almacenados en esta estructura de datos.

```{r message=FALSE, warning=FALSE}
# Elimina los términos compuestos por varias palabras, es decir, cuyos valores
# contengan guiones bajos
sent5_lex_words <- sent5_lex %>% filter(str_detect(CONCEPT, "_", negate=TRUE))
# Lematiza los términos del diccionario y lo convierte en un dataset
sent5_df <- preprocess_dict(sent5_lex_words$CONCEPT, sent5_lex_words$POLARITY)
# Número de términos del diccionario original
length(sent5_lex$CONCEPT)
# Número de términos del diccionario lematizado
length(sent5_df$lem_words)
# Proporción de términos de las distintas categorías
summary(sent5_df$sents)
```

Tras preprocesar el diccionario SenticNet 5.0 considerando únicamente los términos individuales y aplicando un proceso de lematización para maximizar la compatibilidad con la sintaxis de los textos a analizar, a continuación utilizamos la función implementada `predict_from_dict`, proporcionando la lista de términos y sentimientos de esta estructura de datos. En este diccionario también es notable la presencia del fenómeno anterior en el que existe una **preferencia dominante** en la clasificación realizada, aunque en este caso se encuentra a favor de la **clase positiva**, siendo el número de ejemplos categorizados como negativos perjudicialmente ínfimo con un total de trece documentos. El siguiente valor representa que se han podido **encontrar el 57% de términos en el diccionario** para su valoración, una cifra apreciablemente superior a la clasificación de los apartados anteriores. Sin embargo, **la tasa de aciertos es del 50%** lo cual significa que aproximadamente la mitad de los documentos han sido erróneamente clasificados. A diferencia del *MPQA Subjectivity Lexicon* en este caso hemos podido observar un alto porcentaje de coincidencia entre los vocablos de los documentos y del diccionario, aunque la tasa de aciertos ha sido casi veinte puntos menor. Presumiblemente, este resultado puede mejorar realizando análisis sintácticos y morfológicos de los documentos con los que identificar expresiones que enriquecen la información útil para reconocer los sentimientos representados en cada uno de ellos.

```{r}
# Calcula las predicciones de los documentos utilizando las listas de términos
# y sentimientos del diccionario
sent5_results <- predict_from_dict(sent5_df$lem_words, sent5_df$sents)
# Resumen de predicciones
table(sent5_results$preds)
# Calcula el porcentaje de palabras encontradas en el diccionario
sent5_results$n_found_words/sent5_results$n_tot_words
# Calcula la precisión comparando las predicciones con las etiquetas reales
sum(sent5_results$preds == df$Sentiment)/length(sent5_results$preds)
```

# Detección de sentimientos con *Machine Learning*

Una de las aproximaciones más adecuadas para resolver problemas de clasificación consiste en construir **modelos predictivos a partir de conjuntos de datos etiquetados** con el fin de predecir los sentimientos de textos desconocidos. A diferencia del enfoque anterior, son más costosos computacional y temporalmente aunque más flexibles por su mayor independencia al contenido de los documentos. Con el objetivo de proporcionar como entrada el conjunto de textos disponibles en el IMDB dataset, debemos transformar su estructura a una organización compatible con los algoritmos de Aprendizaje Automático. La **matriz de documentos** es el esquema más popularmente utilizado para esta aplicación y se puede generar utilizando la función `DocumentTermMatrix` implementada dentro de la biblioteca `tm`.

```{r}
# Carga la librería que permite dividir una matriz de documentos en
# entrenamiento y test
library(caTools)
# Establece una semilla para que los resultados aleatorios sean reproducibles
set.seed(26)
# Carga la librería que contiene el algoritmo Naive Bayes
library(e1071)
# Carga la librería que contiene el algoritmo Random Forest
library(randomForest)

# Genera una matriz de documentos a partir del corpus preprocesado
doc_matrix <- DocumentTermMatrix(clean_text_corpus) 
# Muestra un resumen del resultado
inspect(doc_matrix)
```

Si observamos los resultados obtenidos, destaca el **notable valor de la métrica Sparsity**, que refleja el porcentaje de ceros almacenados en la matriz. La explicación de este fenómeno se fundamenta en la cantidad de términos caracterizados por una **mínima frecuencia** de aparición en los documentos. Como consecuencia, presumiblemente este tipo de conceptos no serán suficientemente relevantes para ser incluidos en este estudio. Por lo tanto, podemos emplear la función `removeSparseTerms` que permite eliminar aquellas palabras cuyo valor de **Sparsity es mayor a un determinado umbral**, de modo que únicamente se consideran los términos con una frecuencia de ocurrencia razonablemente importante. Tras realizar varios experimentos estableciendo diferentes umbrales de Sparsity, conforme más elevado es el valor más restrictivo es el filtrado, lo que conlleva una disminución drástica del número de conceptos. Estableciendo un umbral del 50% podemos apreciar que el conjunto de palabras analizadas por documento se reduce a únicamente **siete términos**, que se pueden visualizar en la matriz resultante. Sin embargo, con este umbral se consigue **suprimir más de la mitad de ceros** produciendo un decremento altamente notable del valor de Sparsity.

```{r}
# Elimina aquellos términos con un valor de sparsity superior al 50%
reduced_doc_matrix <- removeSparseTerms(doc_matrix, .5)
# Muestra un resumen del resultado
inspect(reduced_doc_matrix)
```

Una vez disponemos de la matriz de documentos preprocesada que representa a la totalidad de los comentarios disponibles en el dataset mediante siete términos, a continuación procedemos a implementar la función `train_and_test_model` que contiene el procedimiento requerido para construir un clasificador. 

1. En primer lugar se convierte la matriz de documentos a un *dataframe* y se añade una nueva columna denominada *doc_id* a partir de la original *Index*, que contiene los **identificadores numéricos que representan a cada uno de los textos** unívocamente. Esta información facilita la selección de las muestras con las que entrenar y validar un modelo de manera aleatoria junto con sus respectivas etiquetas. 

2. La segunda fase consiste en dividir el conjunto de documentos en dos subconjuntos con el **80% de ejemplos para entrenamiento y el 20% restante para validación**. Para ello hacemos uso de la función `sample.split` ubicada en la librería `caTools`, a la que se proporciona la matriz de documentos transformada y el ratio que especifica el número de ejemplos para cada fase de la generación del modelo. Con el objetivo de que los resultados sean reproducibles, al comienzo de esta sección se establece una semilla fija.

3. Finalmente, dependiendo del argumento proporcionado a la función, se construye un clasificador utilizando **Naive Bayes o Random Forest** empleando el conjunto de entrenamiento y validándolo con el conjunto de test, calculando la tasa de aciertos comparando las predicciones realizadas sobre el conjunto de test con sus etiquetas reales.

```{r}
# Función que convierte la matriz de documentos en un dataset para 
# posteriormente dividir el conjunto de datos en entrenamiento y validación
# con los que construir un modelo utilizando Naive Bayes o Random Forest.
# Parámetros de entrada: 'NB' para entrenar un modelo con Naive Bayes o 'RF'
# para emplear Random Forest como algoritmo.
train_and_test_model <- function(algorithm) {
  # Convierte la matriz de documentos en un dataset
  reduced_doc_matrix <- as.data.frame(as.matrix(reduced_doc_matrix))
  # Añade el identificador de cada documento
  reduced_doc_matrix['doc_id'] <- df$Index
  # Genera la división del conjunto de textos reservando el 80% para train
  train_split <- sample.split(reduced_doc_matrix$doc_id, SplitRatio=0.8)
  # Obtiene el conjunto de entrenamiento y sus etiquetas excepto la
  # columna que contiene los identificadores de los documentos
  train <- subset(reduced_doc_matrix, train_split==TRUE) %>% select(-doc_id)
  train_labels <- subset(df$Sentiment, train_split==TRUE)
  # Obtiene el conjunto de validación y sus etiquetas excepto la
  # columna que contiene los identificadores de los documentos
  test <- subset(reduced_doc_matrix, train_split==FALSE) %>% select(-doc_id)
  test_labels <- subset(df$Sentiment, train_split==FALSE)
  # Muestra las dimensiones de sendos conjuntos
  print(dim(train))
  print(length(train_labels))
  print(dim(test))
  print(length(test_labels))
  # Variable que almacena el modelo a entrenar
  model <- NULL
  # Entrena un modelo con Naive Bayes
  if (algorithm == 'NB') {
    model <- naiveBayes(train, train_labels)
  }
  # Entrena un modelo con Random Forest
  else {
    model <- randomForest(train, train_labels)
  }
  # Genera las predicciones sobre el conjunto de test
  preds <- predict(model, test)
  # Muestra el accuracy obtenido
  print(sum(preds == test_labels)/length(test_labels))
  # Muestra la matriz de confusión
  print(table(test_labels, preds))
}
```

En la primera ejecución de la función podemos observar que se reservan 1.600 documentos para entrenamiento y 400 para validación, según el ratio propuesto de 80%-20%, considerando como predictores los siete términos de la matriz de documentos. El algoritmo especificado es **Naive Bayes**, seleccionado por su buen equilibrio entre la velocidad de entrenamiento y el óptimo rendimiento que suele proporcionar en la mayoría de problemas de clasificación. Sin embargo, en este caso el modelo obtenido apenas consigue un **50.25% de precisión**, lo que indica que la clasificación de las instancias desconocidas se caracteriza por haber sido prácticamente aleatoria. Observando la matriz de confusión representada al final de los resultados, es considerablemente notable el **altísimo valor de falsos negativos**, por lo que el clasificador parece tener bastantes inconvenientes en reconocer las muestras relativas a la clase negativa.

```{r}
# Entrenamiento y validación de un modelo con Naive Bayes 
train_and_test_model('NB')
```

La segunda ejecución respeta la misma configuración de entrenamiento y validación comentada anteriormente, tal y como podemos apreciar en los cuatro primeros resultados, aunque en este caso se emplea el algoritmo **Random Forest** por su característica robustez y adaptación a casi cualquier conjunto de datos y problema de clasificación. No obstante, de nuevo la **tasa de aciertos del clasificador es considerablemente baja con un 54%**. A diferencia del modelo anterior, en la **matriz de confusión se aprecia un mayor equilibrio en el número de fallos** durante la clasificación de los ejemplos de test pertenecientes a sendas categorías.

```{r}
# Entrenamiento y validación de un modelo con Random Forest
train_and_test_model('RF')
```

Reflexionando acerca de los pésimos resultados y tras efectuar diversos experimentos, parece que la explicación más plausible a este fenómeno es la **drástica reducción del número de términos** que representan a cada texto en la matriz de documentos. Disminuyendo el **umbral superior de Sparsity a un 95%** podemos apreciar que el número de ceros almacenados incrementa cuantiosamente a la vez que también incrementa el número de vocablos considerados.

```{r}
# Elimina aquellos términos con un valor de sparsity superior al 90%
reduced_doc_matrix <- removeSparseTerms(doc_matrix, .95)
# Muestra un resumen del resultado
inspect(reduced_doc_matrix)
```

Si repetimos el entrenamiento del primer modelo utilizando la misma configuración y algoritmo Naive Bayes, podemos observar en los siguientes resultados que la **precisión incrementa notablemente hasta alcanzar más de un 78.25%**. Con esta tasa de aciertos podemos confirmar que el clasificador obtenido está haciendo uso de los patrones que ha aprendido durante la etapa de entrenamiento para identificar las muestras positivas y negativas. Si bien sigue existiendo un **mayor porcentaje de falsos negativos**, el número de fallos en esta categoría se ha reducido drásticamente gracias a una **mayor representación de términos en la matriz de documentos**.

```{r}
# Entrenamiento y validación de un modelo con Naive Bayes 
train_and_test_model('NB')
```

Una situación similar ocurre al repetir la segunda experimentación empleando el algoritmo Random Forest con la misma configuración de entrenamiento y validación, pero utilizando la nueva matriz de documentos ampliada. Con un mayor número de términos por documento se ha conseguido un modelo con una **capacidad de predicción del 85.75%**, una tasa de aciertos bastante más razonable para considerar que el clasificador es de buena calidad. Como consecuencia la matriz de confusión refleja un **menor número de errores** aunque parece disponer del mismo equilibrio en cuanto al número de fallos cometidos en sendas categorías que fue tan característico del modelo generado con la matriz de documentos compuesta por siete vocablos.

```{r}
# Entrenamiento y validación de un modelo con Random Forest
train_and_test_model('RF')
```

# Comparación de técnicas

En esta última sección se pretende realizar una comparación analítica entre las diferentes herramientas empleadas para la detección de sentimientos basada en un conjunto de textos procedentes de una variante del IMDB dataset. La siguiente tabla recopila las principales métricas resultantes para los tres diccionarios utilizados y los dos algoritmos de *Machine Learning* escogidos con los que construir dos modelos predictivos. 

Tal y como podemos apreciar en los tres primeros registros, todos los diccionarios tras preprocesarlos se encuentran **desequilibrados en favor de la clase negativa**, lo que indica que en su lista de términos y sentimientos asociados predominan los vocablos negativos. Como consecuencia, presumiblemente se favorece la detección de este tipo de palabras por lo que los **resultados finales pueden estar sesgados**, dependiendo de la fuerza asociada al desbalanceo de los diccionarios. Esta característica es **especialmente notable en SentiWordNet 3** con aproximadamente un 20% más de ejemplos negativos y por lo tanto, únicamente consigue un 50% de aciertos debido a la existencia del mismo número de opiniones de sendas categorías. 

Otra de las posibles razones explicativas de los pésimos resultados de clasificación por parte de los diccionarios es la dificultad que conlleva almacenar **todos los posibles términos de un determinado idioma**. Se trata de un proceso sumamente costoso temporal y personalmente por la necesidad de supervisión por parte de personas para identificar cada término con su correspondiente sentimiento. Por lo tanto, esta limitación se presenta como el principal inconveniente relativo a la aplicación de este tipo de técnicas para reconocer sentimientos basados en textos.

| Técnica | Términos positivos | Términos negativos | Términos encontrados | Accuracy |
|---|---|---|---|---|
| MPQA | 4.072 | 2.242 | 24.55% | 69.55% |
| SentiWordNet 3 | 46.171 | 8.858 | 55.68% | 50% |
| SenticNet 5 | 19.400 | 19.608 | 57.77% | 50.55% |
| Naive Bayes | - | - | - | 78.25% |
| Random Forest | - | - | - | 85.75% |

En relación a los algoritmos de Aprendizaje Automático seleccionados para este trabajo, hemos podido apreciar la drástica **dependencia de la tasa de aciertos con respecto al número de términos** representados en la matriz de documentos. Si bien el objetivo consistía en eliminar aquellos vocablos con una ínfima frecuencia de aparición en los documentos, parece que al elevar demasiado esta restricción se perdía información valiosa que ayuda a mejorar la capacidad de predicción de los modelos predictivos. Adicionalmente, visualizando las matrices de confusión de sendos clasificadores hemos podido comprobar que **no existe un número de fallos predominante** de una clase concreta, como sucedía al utilizar los diccionarios. Esta cualidad simboliza una evidente ventaja con respecto a las otras técnicas debido a la independencia de los algoritmos de Aprendizaje Automático con respecto a la lista de términos a considerar. Finalmente cabe destacar el **rendimiento superior presentado por Random Forest** con respecto a Naive Bayes en base a la precisión recogida en la tabla anterior, probablemente fruto de su característica robustez y capacidad de adaptación a casi cualquier tipo de problema y conjunto de datos proporcionado.
