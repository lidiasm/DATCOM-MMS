---
output: pdf_document
---

\centering

![Logo UGR](imgs/logo_ugr.jpg)
\textsc{\\[1cm] \LARGE MINERÍA DE MEDIOS SOCIALES\\[1cm]}
\textsc{\Large MÁSTER EN CIENCIA DE DATOS E INGENIERÍA DE COMPUTADORES\\[1cm]}
\textsc{\Large\bfseries Práctica 1 Bloque II - Minería de Texto \\}
\noindent\rule[-1ex]{\textwidth}{3pt}
\textsc{\Large\bfseries Curso 2021-2022 \\}

\textsc{\\[1cm] \large\bfseries Autora: Lidia Sánchez Mérida \\[1cm]}
![Logo ETSIIT](imgs/etsiit_logo.png)
\textsc{\\[1cm] \large Escuela Técnica Superior de Ingenierías Informática y de Telecomunicación}\\
\textsc{\\[1cm] \large Granada, Mayo de 2022}

\pagebreak

\raggedright

# Descripción del conjunto de datos

El dataset seleccionado para realizar este trabajo práctico es conocido como [**IMDB dataset**](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv). Se trata de un conjunto de datos enfocado a resolver un problema de **clasificación binaria para la detección de sentimientos** basada en textos. En particular, contiene un total de cincuenta trescientosopiniones escritas en inglés sobre diversas películas, junto con su correspondiente etiqueta representando el sentimiento que transmiten con un rango de valores entre positivo o negativo. Como podemos observar en los siguientes resultados, el conjunto se encuentra **balanceado** puesto que dispone del mismo número de muestras para cada categoría.

```{r}
# Carga el conjunto de datos desde el fichero CSV
df <- read.csv("IMDB Dataset.csv")
# Número de filas y columnas
dim(df)
# Número de muestras pertenecientes a cada clase
summary(df$sentiment)
```

# Preprocesamiento de textos

En esta segunda sección se establece como objetivo el **tratamiento y limpieza de las opiniones** almacenadas en la primera columna del dataset. Posteriormente, la información preprocesada podrá ser empleada tanto en técnicas de visualización como en algoritmos de agrupamiento. 

```{r message=FALSE, warning=FALSE}
# Carga la librería para trabajar con corpus de textos
library(tm)
# Carga la librería con la que preprocesar textos
library(stringr)
# Carga la librería que permite aplicar lematización
library(textstem)
# Carga la librería que permite representar una nube de palabras
library(wordcloud)
# Carga la librería que permite personalizar la apariencia de una nube de palabras
library(RColorBrewer)
```

## Generar un corpus 

El primer paso que se pretende realizar en este trabajo consiste en generar un *corpus* que represente el conjunto de textos de este dataset en una **estructura de documentos** que permita analizar y transformar su contenido. Para ello hacemos uso de la librería `tm` que dispone de dos funciones principales, la primera de ellas es `VectorSource` que convierte la columna que almacena las opiniones de los usuarios en un tipo de vector con el que generar el *corpus* mediante una segunda función conocida como `Corpus`.

```{r}
# Convierte la columna de opiniones a un vector fuente para generar un corpus
text_corpus <- Corpus(VectorSource(df$review))
text_corpus
```

## Eliminar caracteres no alfanuméricos

Tras construir un esquema organizado de documentos con todas las opiniones disponibles, a continuación procedemos a eliminar todos aquellos **caracteres especiales, numéricos y signos de puntuación**. Una razón explicativa de esta etapa reside en el insignificante conocimiento útil que aportan este tipo de caracteres en la resolución de problemas de clasificación y *clustering*. Para aplicar este preprocesamiento se ha empleado la función `tm_map`, disponible en la librería mencionada anteriormente, de dos formas distintas:

* El uso por defecto conlleva establecer el tipo de caracteres que se desean eliminar del conjunto de documentos. En este trabajo se han utilizado `removePunctuation` con el que eliminar los signos de puntuación y `removeNumbers` para suprimir los dígitos.

* Adicionalmente este método permite diseñar funciones donde se representan patrones de caracteres que se desean anular de los elementos que componen el *corpus*. Para ello la misma biblioteca proporciona la función `content_transformer`, que en combinación con `gsub`, permiten reemplazar todos aquellos caracteres que no se encuentren dentro del patrón alfanumérico especificado por espacios en blanco.

```{r warning=FALSE}
# Elimina signos de puntuación
clean_text_corpus <- tm_map(text_corpus, removePunctuation)
# Elimina caracteres no alfanuméricos
remove_non_alnum <- content_transformer(function(x) gsub("[^[:alnum:] ]", "", x))
clean_text_corpus <- tm_map(clean_text_corpus, remove_non_alnum)
# Elimina dígitos
clean_text_corpus <- tm_map(clean_text_corpus, removeNumbers)
# Mostramos las diferencias entre algunos caracteres del primer documento 
# original y su homólogo preprocesado
substr(text_corpus[[1]]$content, 1, 70)
substr(clean_text_corpus[[1]]$content, 1, 70)
```

## Normalización y stopwords

En este tercer apartado procedemos a normalizar los caracteres del conjunto de textos convirtiéndolos a **minúsculas** para a continuación eliminar aquellos términos considerados como **palabras vacías**. Con este preprocesamiento se pretende suprimir palabras que no aportan información útil al análisis del contenido del dataset. Algunos ejemplos representativos de esta clase de términos son las preposiciones, artículos, conjunciones, determinantes, pronombres, entre otros. Para aplicar sendas técnicas, utilizamos la función función `tm_map`, que en combinación con `tolower` y `stopwords` permite normalizar los caracteres transformándolos a minúsculas y borrar las palabras vacías existentes en inglés, que es el idioma en el que se encuentran las opiniones. Tras visualizar los textos resultantes, se ha insertado un tercer método consistente en suprimir los **espacios en blanco adicionales** procedentes del preprocesamiento anterior haciendo uso, de nuevo, de la misma función aunque con el argumento `stripWhitespace`

```{r warning=FALSE}
# Convierte todos los caracteres a minúsculas
clean_text_corpus <- tm_map(clean_text_corpus, content_transformer(tolower))
# Elimina stopwords en inglés
clean_text_corpus <- tm_map(clean_text_corpus, 
                      content_transformer(removeWords), stopwords("english"))
# Elimina espacios extra
clean_text_corpus <- tm_map(clean_text_corpus, stripWhitespace)
# Mostramos las diferencias entre algunos caracteres del primer documento 
# original y su homólogo preprocesado
substr(text_corpus[[1]]$content, 1, 70)
substr(clean_text_corpus[[1]]$content, 1, 70)
```

## Lematización

La lematización es una técnica de preprocesamiento cuyo objetivo consiste en realizar un análisis morfológico de la sentencia para **reemplazar cada palabra por su término base**. Existen dos principales diferencias que particularizan esta técnica frente a un método similar conocido como *stemming*. Por un lado, la nueva expresión es totalmente legible puesto que contiene todos sus caracteres, mientras que para seleccionar el sustituto también se considera el significado del texto. Con el fin de aplicar este método se hace referencia a la biblioteca `textstem` que almacena la función `lemmatize_strings` con la que transformar cada término en su raíz. Para emplear este método sobre un *corpus* de documentos podemos seguir utilizando la misma función `tm_map`. Tal y como podemos observar en los resultados, cada sustantivo se encuentra en singular, mientras que cada verbo se redacta en su forma simple.

```{r warning=FALSE}
# Lematización
clean_text_corpus <- tm_map(clean_text_corpus, lemmatize_strings)
# Mostramos las diferencias entre algunos caracteres del primer documento 
# original y su homólogo preprocesado
substr(text_corpus[[1]]$content, 1, 70)
substr(clean_text_corpus[[1]]$content, 1, 70)
```

# Nube de palabras

Una de las visualizaciones más popularmente utilizadas para Minería de Textos se conoce como **nube de palabras**. Su principal objetivo consiste en representar los términos de mayor relevancia en función del **número de apariciones** en los diferentes textos. Por lo tanto, en primer lugar se obtiene el número de ocurrencias de cada término utilizando la función `TermDocumentMatrix` implementada en la librería `tm`. No obstante, debido al altísimo volumen de opiniones almacenadas en el dataset, no es posible realizar este cómputo sobre el *corpus* completo por lo que se considera únicamente un subconjunto de trescientoselementos. A continuación se genera una matriz de frecuencias sumando los valores resultantes de la etapa anterior y se organiza en orden decreciente con el fin de priorizar los términos más populares. Finalmente hacemos referencia a la librería `wordcloud` que contiene una función de una nomenclatura similar con la que representar las palabras más relevantes de los primeros trescientosdocumentos seleccionados.

```{r message=FALSE, warning=FALSE}
# Calcula el número de aparciciones de cada término para trescientosdocumentos
freq_matrix <- as.matrix(TermDocumentMatrix(clean_text_corpus[1:300]))
# Suma las ocurrencias obtenidas para calcular la frecuencia total de cada
# palabra y ordena la matriz resultante de mayor a menor
sorted_freq_matrix <- sort(rowSums(freq_matrix), decreasing=TRUE)
# Genera un dataframe para luego representarlo en una nuble de términos
df_freq_matrix <- data.frame(word=names(sorted_freq_matrix), 
                             freq=sorted_freq_matrix)
# Representa una nube de palabras estableciendo el tamaño como la 
# frecuencia, con un mínimo de una aparición en un documento y un máximo
# número de doscientas palabras
wordcloud(words=df_freq_matrix$word, freq=df_freq_matrix$freq, 
          min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

Tal y como podemos apreciar en el gráfico anterior, los términos de mayor tamaño son los que disponen de una mayor relevancia puesto que se caracterizan por un número de ocurrencias superior al resto. Si los observamos podemos comprobar que se trata de **sustantivos y verbos relacionados con la temática** del conjunto de datos, como por ejemplo *movie*, *film*, *see*. Adicionalmente, existen algunos conceptos ligeramente menos relevantes que expresan las emociones vinculadas a las opiniones redactadas por los usuarios: los **adjetivos**. Parece ser que los **más frecuentes tienen una connotación positiva** por lo que, a pesar de que el dataset se encuentra balanceado en el número de ejemplos de sendas clases, la tendencia de los participantes se ha inclinado por el uso de más términos positivos. Finalmente, un aspecto a destacar reside en la aparición de una minoría de **dígitos expresados como texto** que no han sido detectados por las técnicas de preprocesamiento. Una de las librerías que se pueden emplear para transformar las cifras manuscritas en valores numéricos es `wordstonumbers`, sin embargo no se encuentra disponible para una versión de R igual o superior a la 3.6.3, por lo que ha sido imposible su aplicación.

# Agrupamiento de textos

```{r}
dist.matrix = proxy::dist(as.matrix(freq_matrix), method = "cosine")
clustering.hierarchical <- hclust(dist.matrix, method = "ward.D2") 
plot(clustering.hierarchical, hang = -1)
rect.hclust(clustering.hierarchical, k = 6, border = "red")
```


# Clasificación de sentimientos

